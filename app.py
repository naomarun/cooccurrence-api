#!/usr/bin/env python3
"""
å…±èµ·èªæŠ½å‡ºAPIã‚µãƒ¼ãƒãƒ¼ï¼ˆValueSERPå¯¾å¿œç‰ˆ - æœ€çµ‚ç‰ˆï¼‰
MeCabã‚’ä½¿ã£ãŸå½¢æ…‹ç´ è§£æã«ã‚ˆã‚‹çœŸã®å…±èµ·èªæŠ½å‡º
æ—¥æœ¬èªæ¤œç´¢æœ€é©åŒ–ã€AI Overviewã¯é™¤å¤–
"""

import os
import requests
import time
from bs4 import BeautifulSoup
from collections import Counter
import re
from urllib.parse import urlencode
import json
from flask import Flask, request, jsonify
from flask_cors import CORS
import MeCab

app = Flask(__name__)
CORS(app)

# ç’°å¢ƒå¤‰æ•°ã‹ã‚‰å–å¾—
AHREFS_API_KEY = os.environ.get("AHREFS_API_KEY", "")
VALUESERP_API_KEY = os.environ.get("VALUESERP_API_KEY", "")

# MeCabã®åˆæœŸåŒ–
try:
    mecab = MeCab.Tagger("-Owakati")
    print("âœ… MeCabåˆæœŸåŒ–æˆåŠŸ")
except Exception as e:
    print(f"âš ï¸  MeCabåˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼: {e}")
    mecab = None


# å›½åˆ¥è¨­å®š
COUNTRY_CONFIG = {
    'jp': {
        'location': 'Japan',
        'google_domain': 'google.co.jp',
        'gl': 'jp',
        'hl': 'ja'
    },
    'us': {
        'location': 'United States',
        'google_domain': 'google.com',
        'gl': 'us',
        'hl': 'en'
    },
    'uk': {
        'location': 'United Kingdom',
        'google_domain': 'google.co.uk',
        'gl': 'uk',
        'hl': 'en'
    },
    'ca': {
        'location': 'Canada',
        'google_domain': 'google.ca',
        'gl': 'ca',
        'hl': 'en'
    },
    'au': {
        'location': 'Australia',
        'google_domain': 'google.com.au',
        'gl': 'au',
        'hl': 'en'
    },
    'de': {
        'location': 'Germany',
        'google_domain': 'google.de',
        'gl': 'de',
        'hl': 'de'
    },
    'fr': {
        'location': 'France',
        'google_domain': 'google.fr',
        'gl': 'fr',
        'hl': 'fr'
    },
    'kr': {
        'location': 'South Korea',
        'google_domain': 'google.co.kr',
        'gl': 'kr',
        'hl': 'ko'
    },
    'cn': {
        'location': 'China',
        'google_domain': 'google.com.hk',
        'gl': 'cn',
        'hl': 'zh-CN'
    }
}


def get_top_ranking_pages_valueserp(keyword, country="jp", limit=10):
    """ValueSERP APIã§ä¸Šä½ãƒ©ãƒ³ã‚­ãƒ³ã‚°ãƒšãƒ¼ã‚¸ã‚’å–å¾—ï¼ˆæ—¥æœ¬èªæœ€é©åŒ–ã€AI Overviewé™¤å¤–ï¼‰"""
    print(f"ğŸ” ValueSERP APIã§ä¸Šä½ãƒšãƒ¼ã‚¸ã‚’å–å¾—ä¸­: {keyword}")
    
    # å›½åˆ¥è¨­å®šã‚’å–å¾—
    config = COUNTRY_CONFIG.get(country, COUNTRY_CONFIG['jp'])
    
    params = {
        'api_key': VALUESERP_API_KEY,
        'q': keyword,
        'location': config['location'],
        'google_domain': config['google_domain'],
        'gl': config['gl'],
        'hl': config['hl'],
        'output': 'json',
        'num': limit,
        'include_ai_overview': 'false'  # AI Overviewã¯æ˜ç¤ºçš„ã«é™¤å¤–
    }
    
    try:
        print(f"ğŸ“¤ ãƒªã‚¯ã‚¨ã‚¹ãƒˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿: {json.dumps({k: v for k, v in params.items() if k != 'api_key'}, ensure_ascii=False)}")
        
        response = requests.get('https://api.valueserp.com/search', params=params, timeout=60)
        
        print(f"ğŸ“¥ ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚³ãƒ¼ãƒ‰: {response.status_code}")
        
        response.raise_for_status()
        
        data = response.json()
        
        # ãƒ‡ãƒãƒƒã‚°æƒ…å ±ã‚’å‡ºåŠ›
        print(f"ğŸ“Š ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚­ãƒ¼: {list(data.keys())}")
        
        top_urls = []
        
        # ValueSERPã®ãƒ¬ã‚¹ãƒãƒ³ã‚¹æ§‹é€ ã‚’ç¢ºèª
        if data.get('request_info', {}).get('success') == True:
            if 'organic_results' in data:
                print(f"ğŸ“Š organic_resultsé…åˆ—ã®é•·ã•: {len(data['organic_results'])}")
                
                for i, result in enumerate(data['organic_results'], 1):
                    if 'link' in result:
                        top_urls.append(result['link'])
                        print(f"  {i}ä½: {result['link']}")
                        if len(top_urls) >= limit:
                            break
            else:
                print("âš ï¸  'organic_results'ã‚­ãƒ¼ãŒãƒ¬ã‚¹ãƒãƒ³ã‚¹ã«å­˜åœ¨ã—ã¾ã›ã‚“")
        else:
            print(f"âš ï¸  APIãƒªã‚¯ã‚¨ã‚¹ãƒˆãŒå¤±æ•—ã—ã¾ã—ãŸ: {data.get('request_info', {}).get('message', 'ä¸æ˜ãªã‚¨ãƒ©ãƒ¼')}")
        
        print(f"âœ… ä¸Šä½ãƒšãƒ¼ã‚¸å–å¾—å®Œäº†: {len(top_urls)}ä»¶")
        
        if len(top_urls) == 0:
            print(f"âš ï¸  è­¦å‘Š: ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã€Œ{keyword}ã€ã®æ¤œç´¢çµæœãŒ0ä»¶ã§ã—ãŸ")
        
        return top_urls
    
    except requests.exceptions.RequestException as e:
        print(f"âš ï¸  ValueSERP API ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
        return []
    except Exception as e:
        print(f"âš ï¸  äºˆæœŸã›ã¬ã‚¨ãƒ©ãƒ¼: {e}")
        import traceback
        traceback.print_exc()
        return []


def get_top_ranking_pages(keyword, country="jp", limit=10):
    """Ahrefs APIã§ä¸Šä½ãƒ©ãƒ³ã‚­ãƒ³ã‚°ãƒšãƒ¼ã‚¸ã‚’å–å¾—"""
    print(f"ğŸ” Ahrefs APIã§ä¸Šä½ãƒšãƒ¼ã‚¸ã‚’å–å¾—ä¸­: {keyword}")
    
    url = "https://api.ahrefs.com/v3/serp-overview/serp-overview"
    
    headers = {
        "Authorization": f"Bearer {AHREFS_API_KEY}",
        "Accept": "application/json"
    }
    
    params = {
        "keyword": keyword,
        "country": country,
        "select": "position,url,title,type",
        "top_positions": limit
    }
    
    try:
        full_url = f"{url}?{urlencode(params)}"
        
        print(f"ğŸ“¤ ãƒªã‚¯ã‚¨ã‚¹ãƒˆURL: {full_url}")
        
        response = requests.get(full_url, headers=headers, timeout=30)
        
        print(f"ğŸ“¥ ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚³ãƒ¼ãƒ‰: {response.status_code}")
        
        if response.status_code == 200:
            data = response.json()
            
            top_urls = []
            if 'positions' in data:
                for result in data['positions']:
                    if 'url' in result and result['url'] is not None:
                        result_types = result.get('type', [])
                        if isinstance(result_types, str):
                            result_types = [result_types]
                        
                        # AI Overviewã‚’é™¤å¤–
                        if result_types == ['ai_overview']:
                            continue
                        
                        top_urls.append(result['url'])
                        if len(top_urls) >= limit:
                            break
            
            print(f"âœ… ä¸Šä½ãƒšãƒ¼ã‚¸å–å¾—å®Œäº†: {len(top_urls)}ä»¶")
            return top_urls
        
        else:
            print(f"âš ï¸  Ahrefs API ã‚¨ãƒ©ãƒ¼: {response.status_code}")
            print(f"ãƒ¬ã‚¹ãƒãƒ³ã‚¹: {response.text}")
            return []
    
    except Exception as e:
        print(f"âš ï¸  ä¸Šä½ãƒšãƒ¼ã‚¸å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
        import traceback
        traceback.print_exc()
        return []


def get_top_ranking_pages_hybrid(keyword, country="jp", limit=10):
    """Ahrefsã‚’å„ªå…ˆã—ã€å¤±æ•—æ™‚ã«ValueSERPã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã™ã‚‹"""
    
    # 1. ã¾ãšAhrefs APIã‚’è©¦ã™ï¼ˆAPIã‚­ãƒ¼ãŒè¨­å®šã•ã‚Œã¦ã„ã‚‹å ´åˆã®ã¿ï¼‰
    if AHREFS_API_KEY:
        print("ğŸ” Ahrefs APIã§è©¦è¡Œä¸­...")
        ahrefs_urls = get_top_ranking_pages(keyword, country, limit)
        
        if ahrefs_urls:
            print("âœ… Ahrefs APIã§å–å¾—æˆåŠŸ")
            return ahrefs_urls, 'ahrefs'
    
    # 2. Ahrefsã§å¤±æ•—ã—ãŸå ´åˆã€ã¾ãŸã¯APIã‚­ãƒ¼ãŒãªã„å ´åˆã€ValueSERP APIã‚’å‘¼ã³å‡ºã™
    if VALUESERP_API_KEY:
        print("âš ï¸  Ahrefsã§çµæœãªã—ã€ã¾ãŸã¯APIã‚­ãƒ¼æœªè¨­å®šã€‚ValueSERPã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã—ã¾ã™...")
        valueserp_urls = get_top_ranking_pages_valueserp(keyword, country, limit)
        return valueserp_urls, 'valueserp'
    
    print("âš ï¸  ã‚¨ãƒ©ãƒ¼: Ahrefsã¨ValueSERPã®ä¸¡æ–¹ã®APIã‚­ãƒ¼ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
    return [], 'none'


def scrape_page_content(url, timeout=10):
    """æŒ‡å®šURLã®ãƒšãƒ¼ã‚¸ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°"""
    try:
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"
        }
        
        response = requests.get(url, headers=headers, timeout=timeout)
        response.raise_for_status()
        
        soup = BeautifulSoup(response.content, 'html.parser')
        
        for element in soup(['script', 'style', 'nav', 'header', 'footer', 'aside']):
            element.decompose()
        
        main_content = (
            soup.find('article') or 
            soup.find('main') or 
            soup.find('div', class_=re.compile(r'content|article|post', re.I)) or
            soup.find('body')
        )
        
        if main_content:
            text = main_content.get_text(separator=' ', strip=True)
            text = re.sub(r'\s+', ' ', text)
            return text
        
        return ""
    
    except Exception as e:
        print(f"  âš ï¸  ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚¨ãƒ©ãƒ¼ ({url}): {e}")
        return ""


def extract_cooccurrence_with_mecab(texts, keyword, top_n=50):
    """MeCabã‚’ä½¿ã£ãŸå…±èµ·èªæŠ½å‡º"""
    print(f"ğŸ“Š MeCabã§å…±èµ·èªã‚’æŠ½å‡ºä¸­...")
    
    if not mecab:
        print("âš ï¸  MeCabãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚ç°¡æ˜“æŠ½å‡ºã‚’ä½¿ç”¨ã—ã¾ã™ã€‚")
        return extract_cooccurrence_simple(texts, keyword, top_n)
    
    combined_text = ' '.join(texts)
    
    # MeCabã§å½¢æ…‹ç´ è§£æ
    try:
        parsed = mecab.parse(combined_text)
        words = parsed.split()
    except Exception as e:
        print(f"âš ï¸  MeCabè§£æã‚¨ãƒ©ãƒ¼: {e}")
        return extract_cooccurrence_simple(texts, keyword, top_n)
    
    # ã‚¹ãƒˆãƒƒãƒ—ãƒ¯ãƒ¼ãƒ‰
    stopwords = {
        'ã“ã¨', 'ãŸã‚', 'ã‚‚ã®', 'ã“ã‚Œ', 'ãã‚Œ', 'ã‚ã‚Œ', 'ã“ã®', 'ãã®', 'ã‚ã®',
        'ã“ã“', 'ãã“', 'ã‚ãã“', 'ã§ã™', 'ã¾ã™', 'ã‚ã‚‹', 'ã„ã‚‹', 'ãªã‚‹', 'ã™ã‚‹',
        'ã§ãã‚‹', 'ã¨ã„ã†', 'ã¨ã—ã¦', 'ã«ã‚ˆã‚Š', 'ã«ã¤ã„ã¦', 'ã«ãŠã„ã¦', 'ã«å¯¾ã—ã¦',
        'ã®', 'ã«', 'ã‚’', 'ã¯', 'ãŒ', 'ã§', 'ã¨', 'ã‚‚', 'ã‹ã‚‰', 'ã¾ã§', 'ã‚ˆã‚Š',
        'ã¸', 'ã‚„', 'ã‹', 'ã­', 'ã‚ˆ', 'ãª', 'ã ', 'ãŸ', 'ã¦', 'ã‚Œ', 'ã°'
    }
    
    # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚‚ã‚¹ãƒˆãƒƒãƒ—ãƒ¯ãƒ¼ãƒ‰ã«è¿½åŠ 
    keyword_words = set(re.findall(r'\w+', keyword))
    stopwords.update(keyword_words)
    
    # ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ï¼ˆ2æ–‡å­—ä»¥ä¸Šã€ã‚¹ãƒˆãƒƒãƒ—ãƒ¯ãƒ¼ãƒ‰é™¤å¤–ï¼‰
    filtered_words = [
        w for w in words 
        if len(w) >= 2 and w not in stopwords and re.search(r'[\u4e00-\u9fff\u3040-\u309f\u30a0-\u30ff]', w)
    ]
    
    # å‡ºç¾å›æ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆ
    word_counts = Counter(filtered_words)
    
    # ä¸Šä½Nä»¶ã‚’å–å¾—
    top_words = word_counts.most_common(top_n)
    
    print(f"âœ… å…±èµ·èªæŠ½å‡ºå®Œäº†: {len(top_words)}ä»¶")
    
    return top_words


def extract_cooccurrence_simple(texts, keyword, top_n=50):
    """ç°¡æ˜“çš„ãªå…±èµ·èªæŠ½å‡ºï¼ˆMeCabä¸ä½¿ç”¨ï¼‰"""
    combined_text = ' '.join(texts)
    
    # 2ã€œ4æ–‡å­—ã®æ—¥æœ¬èªãƒ•ãƒ¬ãƒ¼ã‚ºã‚’æŠ½å‡º
    words = re.findall(r'[\u4e00-\u9fff\u3040-\u309f\u30a0-\u30ff]{2,4}', combined_text)
    
    stopwords = {
        'ã“ã¨', 'ãŸã‚', 'ã‚‚ã®', 'ã“ã‚Œ', 'ãã‚Œ', 'ã‚ã‚Œ', 'ã“ã®', 'ãã®', 'ã‚ã®',
        'ã“ã“', 'ãã“', 'ã‚ãã“', 'ã§ã™', 'ã¾ã™', 'ã‚ã‚‹', 'ã„ã‚‹', 'ãªã‚‹', 'ã™ã‚‹',
        'ã§ãã‚‹', 'ã¨ã„ã†', 'ã¨ã—ã¦', 'ã«ã‚ˆã‚Š', 'ã«ã¤ã„ã¦', 'ã«ãŠã„ã¦', 'ã«å¯¾ã—ã¦'
    }
    
    keyword_words = set(re.findall(r'\w+', keyword))
    stopwords.update(keyword_words)
    
    filtered_words = [w for w in words if w not in stopwords]
    word_counts = Counter(filtered_words)
    top_words = word_counts.most_common(top_n)
    
    return top_words


@app.route('/health', methods=['GET'])
def health_check():
    """ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ"""
    return jsonify({
        'status': 'ok',
        'mecab_available': mecab is not None,
        'ahrefs_api_configured': bool(AHREFS_API_KEY),
        'valueserp_api_configured': bool(VALUESERP_API_KEY)
    })


@app.route('/extract', methods=['POST'])
def extract_cooccurrence():
    """å…±èµ·èªæŠ½å‡ºã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ"""
    try:
        data = request.get_json()
        
        if not data or 'keyword' not in data:
            return jsonify({'error': 'ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒæŒ‡å®šã•ã‚Œã¦ã„ã¾ã›ã‚“'}), 400
        
        keyword = data['keyword']
        country = data.get('country', 'jp')
        top_pages = data.get('top_pages', 10)
        top_words = data.get('top_words', 50)
        use_api = data.get('use_api', 'hybrid')  # 'ahrefs', 'valueserp', 'hybrid'
        
        print(f"\n{'='*60}")
        print(f"å…±èµ·èªæŠ½å‡ºãƒªã‚¯ã‚¨ã‚¹ãƒˆ: {keyword}")
        print(f"ä½¿ç”¨API: {use_api}")
        print(f"å–å¾—ãƒšãƒ¼ã‚¸æ•°: 1ã€œ{top_pages}ä½")
        print(f"{'='*60}\n")
        
        # 1. ä¸Šä½ãƒšãƒ¼ã‚¸ã®URLå–å¾—
        if use_api == 'valueserp':
            top_urls = get_top_ranking_pages_valueserp(keyword, country, top_pages)
            api_used = 'valueserp'
        elif use_api == 'ahrefs':
            top_urls = get_top_ranking_pages(keyword, country, top_pages)
            api_used = 'ahrefs'
        else:  # hybrid
            top_urls, api_used = get_top_ranking_pages_hybrid(keyword, country, top_pages)
        
        if not top_urls:
            return jsonify({
                'error': 'ä¸Šä½ãƒšãƒ¼ã‚¸ãŒå–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸ',
                'keyword': keyword,
                'cooccurrence_words': [],
                'analyzed_pages': 0,
                'api_used': api_used,
                'debug_info': {
                    'message': 'APIã‹ã‚‰0ä»¶ã®çµæœãŒè¿”ã•ã‚Œã¾ã—ãŸã€‚ä¸Šè¨˜ã®ãƒ­ã‚°ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚'
                }
            }), 500
        
        # 2. å„ãƒšãƒ¼ã‚¸ã®ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°
        print(f"\nğŸ“¥ {len(top_urls)}ãƒšãƒ¼ã‚¸ã®ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ä¸­...")
        texts = []
        
        for i, url in enumerate(top_urls, 1):
            print(f"  [{i}/{len(top_urls)}] {url}")
            content = scrape_page_content(url)
            
            if content:
                texts.append(content)
                print(f"    âœ… å–å¾—æˆåŠŸ ({len(content)}æ–‡å­—)")
            else:
                print(f"    âš ï¸  å–å¾—å¤±æ•—")
            
            time.sleep(1)
        
        if not texts:
            return jsonify({
                'error': 'ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãŒå–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸ',
                'keyword': keyword,
                'cooccurrence_words': [],
                'analyzed_pages': 0,
                'api_used': api_used
            }), 500
        
        print(f"\nâœ… {len(texts)}ãƒšãƒ¼ã‚¸ã®ã‚³ãƒ³ãƒ†ãƒ³ãƒ„å–å¾—å®Œäº†")
        
        # 3. å…±èµ·èªæŠ½å‡ºï¼ˆMeCabä½¿ç”¨ï¼‰
        cooccurrence_words = extract_cooccurrence_with_mecab(texts, keyword, top_words)
        
        # 4. çµæœã‚’æ•´å½¢
        result_list = [word for word, count in cooccurrence_words]
        result_str = ', '.join(result_list)
        
        print(f"\n{'='*60}")
        print(f"âœ… å…±èµ·èªæŠ½å‡ºå®Œäº†!")
        print(f"{'='*60}")
        print(f"\nã€æŠ½å‡ºã•ã‚ŒãŸå…±èµ·èª (ä¸Šä½20ä»¶)ã€‘")
        for i, (word, count) in enumerate(cooccurrence_words[:20], 1):
            print(f"  {i:2d}. {word:20s} ({count:3d}å›)")
        
        return jsonify({
            'keyword': keyword,
            'cooccurrence_words': result_list,
            'cooccurrence_string': result_str,
            'analyzed_pages': len(texts),
            'top_urls': top_urls,
            'mecab_used': mecab is not None,
            'api_used': api_used
        })
    
    except Exception as e:
        print(f"âš ï¸  ã‚¨ãƒ©ãƒ¼: {e}")
        import traceback
        traceback.print_exc()
        return jsonify({'error': str(e)}), 500


if __name__ == '__main__':
    print("\n" + "="*60)
    print("å…±èµ·èªæŠ½å‡ºAPIã‚µãƒ¼ãƒãƒ¼èµ·å‹•ä¸­ï¼ˆæœ€çµ‚ç‰ˆï¼‰...")
    print("="*60)
    print(f"MeCab: {'âœ… åˆ©ç”¨å¯èƒ½' if mecab else 'âš ï¸  åˆ©ç”¨ä¸å¯'}")
    print(f"Ahrefs API: {'âœ… è¨­å®šæ¸ˆã¿' if AHREFS_API_KEY else 'âš ï¸  æœªè¨­å®š'}")
    print(f"ValueSERP API: {'âœ… è¨­å®šæ¸ˆã¿' if VALUESERP_API_KEY else 'âš ï¸  æœªè¨­å®š'}")
    print(f"å¯¾å¿œå›½æ•°: {len(COUNTRY_CONFIG)}ãƒ¶å›½")
    print(f"AI Overview: âŒ é™¤å¤–ï¼ˆé€šå¸¸ã®æ¤œç´¢çµæœã®ã¿ï¼‰")
    print("="*60 + "\n")
    
    app.run(host='0.0.0.0', port=5000, debug=False)
